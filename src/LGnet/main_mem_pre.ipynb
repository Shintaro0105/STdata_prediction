{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as utils\n",
    "from Discriminator import *\n",
    "from LGnet_mem import *\n",
    "\n",
    "\n",
    "def wasserstein_loss(y_pred, y_true):\n",
    "    return torch.mean(y_pred * y_true)\n",
    "\n",
    "\n",
    "def plot_locals_graph(local_statistics, epoch):\n",
    "    local_values = local_statistics[:, :].cpu().detach().numpy()\n",
    "    num_dimensions = local_values.shape[1]\n",
    "    num_batchs = local_values.shape[0]\n",
    "    colors = plt.cm.get_cmap('tab10', num_batchs) \n",
    "\n",
    "    indices = np.arange(num_dimensions)\n",
    "    for i, idx in enumerate(range(num_batchs)):\n",
    "        if idx < num_batchs:\n",
    "            values_index = local_values[idx, :]\n",
    "            plt.plot(indices, values_index, linestyle='-', label=f'Values at Batch ID {i}', \n",
    "                        color=colors(i), alpha=0.5)\n",
    "\n",
    "    plt.xlabel('id')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'Local Values at epoch {epoch}')\n",
    "    #plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_local_input_graph(z, z_prime, x_i, batch_id, epoch):\n",
    "    z_values = z[batch_id, :].cpu().detach().numpy()\n",
    "    z_prime_values = z_prime[batch_id, :].cpu().detach().numpy()\n",
    "    x_i_values = x_i[batch_id, :].cpu().detach().numpy()\n",
    "    num_dimensions = z_values.shape[0]\n",
    "\n",
    "    indices = np.arange(num_dimensions)\n",
    "      \n",
    "    plt.plot(indices, z_values, linestyle='-', label='z', color='r', alpha=0.5)\n",
    "    plt.plot(indices, z_prime_values, linestyle='-', label='z_prime', color='b', alpha=0.5)  \n",
    "    plt.plot(indices, x_i_values, linestyle='-', label='x_i', color='g', alpha=0.5)\n",
    "\n",
    "    plt.xlabel('id')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'Local input Values at Batch ID {batch_id} and epoch {epoch}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_losses_combined(losses_train, losses_valid, losses_d_real, losses_d_fake, filename):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.plot(losses_train, label=\"Training Loss\")\n",
    "    plt.plot(losses_valid, label=\"Validation Loss\")\n",
    "    plt.plot(losses_d_real, label=\"Discriminator Real Loss\")\n",
    "    plt.plot(losses_d_fake, label=\"Discriminator Fake Loss\")\n",
    "\n",
    "    plt.title(\"Losses Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def PrepareDataset(\n",
    "    speed_matrix,\n",
    "    BATCH_SIZE=40,\n",
    "    seq_len=10,\n",
    "    pred_len=1,\n",
    "    train_propotion=0.7,\n",
    "    valid_propotion=0.2,\n",
    "    masking=False,\n",
    "):\n",
    "    \"\"\"Prepare training and testing datasets and dataloaders.\n",
    "\n",
    "    Convert speed/volume/occupancy matrix to training and testing dataset.\n",
    "    The vertical axis of speed_matrix is the time axis and the horizontal axis\n",
    "    is the spatial axis.\n",
    "\n",
    "    Args:\n",
    "        speed_matrix: a Matrix containing spatial-temporal speed data for a network\n",
    "        seq_len: length of input sequence\n",
    "        pred_len: length of predicted sequence\n",
    "    Returns:\n",
    "        Training dataloader\n",
    "        Testing dataloader\n",
    "    \"\"\"\n",
    "\n",
    "    speed_matrix_s = np.array_split(speed_matrix, 16)\n",
    "    speed_matrix = speed_matrix_s[0]\n",
    "    time_len = speed_matrix.shape[0]\n",
    "    print(\"Time len: \", time_len)\n",
    "\n",
    "    speed_matrix = speed_matrix.clip(0, 100)\n",
    "\n",
    "    max_speed = speed_matrix.max().max()\n",
    "    speed_matrix = speed_matrix / max_speed\n",
    "\n",
    "    speed_sequences, speed_labels = [], []\n",
    "    for i in range(time_len - seq_len - pred_len):\n",
    "        speed_sequences.append(speed_matrix.iloc[i : i + seq_len].values)\n",
    "        speed_labels.append(speed_matrix.iloc[i + seq_len : i + seq_len + pred_len].values)\n",
    "    speed_sequences, speed_labels = np.asarray(speed_sequences), np.asarray(speed_labels)\n",
    "\n",
    "    # using zero-one mask to randomly set elements to zeros\n",
    "    if masking:\n",
    "        print(\"Split Speed finished. Start to generate Mask, Delta, Last_observed_X ...\")\n",
    "        Mask = np.where(speed_sequences == 0, 0, 1)\n",
    "        Mask_l = np.where(speed_labels == 0, 0, 1)\n",
    "        speed_sequences = np.multiply(speed_sequences, Mask)\n",
    "\n",
    "        # temporal information\n",
    "        interval = 5  # 5 minutes\n",
    "        S = np.zeros_like(speed_sequences)  # time stamps\n",
    "        for i in range(S.shape[1]):\n",
    "            S[:, i, :] = interval * i\n",
    "\n",
    "        Delta = np.zeros_like(speed_sequences)  # time intervals\n",
    "        for i in range(1, S.shape[1]):\n",
    "            Delta[:, i, :] = S[:, i, :] - S[:, i - 1, :]\n",
    "\n",
    "        # Calculate Delta_b (backward direction)\n",
    "        Delta_b = np.zeros_like(speed_sequences)\n",
    "        for i in range(0, S.shape[1] - 1):\n",
    "            Delta_b[:, i, :] = S[:, i + 1, :] - S[:, i, :]\n",
    "\n",
    "        missing_index = np.where(Mask == 0)\n",
    "\n",
    "        X_last_obsv = np.copy(speed_sequences)\n",
    "        X_last_obsv_b = np.copy(speed_sequences)\n",
    "        for idx in range(missing_index[0].shape[0]):\n",
    "            i = missing_index[0][idx]\n",
    "            j = missing_index[1][idx]\n",
    "            k = missing_index[2][idx]\n",
    "            if j != 0 and j != 9:\n",
    "                Delta[i, j + 1, k] = Delta[i, j + 1, k] + Delta[i, j, k]\n",
    "            if j != 0:\n",
    "                X_last_obsv[i, j, k] = X_last_obsv[i, j - 1, k]  # last observation\n",
    "\n",
    "        Delta = Delta / Delta.max()  # normalize\n",
    "\n",
    "        for idx in range(missing_index[0].shape[0], -1):\n",
    "            i = missing_index[0][idx]\n",
    "            j = missing_index[1][idx]\n",
    "            k = missing_index[2][idx]\n",
    "            if j != 0 and j != 9:\n",
    "                Delta_b[i, j - 1, k] += Delta_b[i, j, k] + Delta_b[i, j - 1, k]\n",
    "            if j != 9:\n",
    "                X_last_obsv_b[i, j, k] = X_last_obsv_b[i, j + 1, k]\n",
    "\n",
    "        Delta_b = Delta_b / Delta_b.max()  # normalize\n",
    "\n",
    "    # shuffle and split the dataset to training and testing datasets\n",
    "    print(\"Generate Mask, Delta, Last_observed_X finished. Start to shuffle and split dataset ...\")\n",
    "    sample_size = speed_sequences.shape[0]\n",
    "    index = np.arange(sample_size, dtype=int)\n",
    "    # np.random.seed(1024)\n",
    "    # np.random.shuffle(index)\n",
    "\n",
    "    speed_sequences = speed_sequences[index]\n",
    "    speed_labels = speed_labels[index]\n",
    "\n",
    "    if masking:\n",
    "        X_last_obsv = X_last_obsv[index]\n",
    "        Mask = Mask[index]\n",
    "        Delta = Delta[index]\n",
    "        X_last_obsv_b = X_last_obsv_b[index]\n",
    "        Delta_b = Delta_b[index]\n",
    "\n",
    "        speed_sequences = np.expand_dims(speed_sequences, axis=1)\n",
    "        X_last_obsv = np.expand_dims(X_last_obsv, axis=1)\n",
    "        Mask = np.expand_dims(Mask, axis=1)\n",
    "        Delta = np.expand_dims(Delta, axis=1)\n",
    "        X_last_obsv_b = np.expand_dims(X_last_obsv_b, axis=1)\n",
    "        Delta_b = np.expand_dims(Delta_b, axis=1)\n",
    "\n",
    "        dataset_agger = np.concatenate((speed_sequences, X_last_obsv, Mask, Delta, X_last_obsv_b, Delta_b), axis=1)\n",
    "\n",
    "        speed_labels = np.expand_dims(speed_labels, axis=1)\n",
    "        speed_labels_mask = np.expand_dims(Mask_l, axis=1)\n",
    "\n",
    "        speed_labels = np.concatenate((speed_labels, speed_labels_mask), axis=1)\n",
    "\n",
    "    train_index = int(np.floor(sample_size * train_propotion))\n",
    "    valid_index = int(np.floor(sample_size * (train_propotion + valid_propotion)))\n",
    "\n",
    "    if masking:\n",
    "        train_data, train_label = dataset_agger[:train_index], speed_labels[:train_index]\n",
    "        valid_data, valid_label = dataset_agger[train_index:valid_index], speed_labels[train_index:valid_index]\n",
    "        test_data, test_label = dataset_agger[valid_index:], speed_labels[valid_index:]\n",
    "    else:\n",
    "        train_data, train_label = speed_sequences[:train_index], speed_labels[:train_index]\n",
    "        valid_data, valid_label = speed_sequences[train_index:valid_index], speed_labels[train_index:valid_index]\n",
    "        test_data, test_label = speed_sequences[valid_index:], speed_labels[valid_index:]\n",
    "\n",
    "    train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n",
    "    valid_data, valid_label = torch.Tensor(valid_data), torch.Tensor(valid_label)\n",
    "    test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
    "\n",
    "    train_dataset = utils.TensorDataset(train_data, train_label)\n",
    "    valid_dataset = utils.TensorDataset(valid_data, valid_label)\n",
    "    test_dataset = utils.TensorDataset(test_data, test_label)\n",
    "\n",
    "    train_dataloader = utils.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    valid_dataloader = utils.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    test_dataloader = utils.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "    X_mean = np.mean(speed_sequences, axis=0)\n",
    "\n",
    "    print(\"Finished\")\n",
    "\n",
    "    return train_dataloader, valid_dataloader, test_dataloader, max_speed, X_mean\n",
    "\n",
    "\n",
    "def Train_Model(\n",
    "    model, discriminator, train_dataloader, valid_dataloader, num_epochs=300, patience=10, min_delta=0.00001\n",
    "):\n",
    "    print(\"Model Structure: \", model)\n",
    "    print(\"Start Training ... \")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    discriminator.to(device)\n",
    "\n",
    "    if type(model) == nn.modules.container.Sequential:\n",
    "        output_last = model[-1].output_last\n",
    "        print(\"Output type dermined by the last layer\")\n",
    "    else:\n",
    "        output_last = model.output_last\n",
    "        print(\"Output type dermined by the model\")\n",
    "\n",
    "    loss_MSE = torch.nn.MSELoss()\n",
    "    loss_L1 = torch.nn.L1Loss()\n",
    "\n",
    "    lambda_dis = 10.0\n",
    "    learning_rate = 0.0001\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    optimizer_adv = torch.optim.RMSprop(discriminator.parameters(), lr=learning_rate)\n",
    "    adversarial_loss = wasserstein_loss\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "\n",
    "    interval = 100\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "    losses_epochs_train = []\n",
    "    losses_epochs_valid = []\n",
    "    losses_epochs_d_loss_real = []\n",
    "    losses_epochs_d_loss_fake = []\n",
    "\n",
    "    cur_time = time.time()\n",
    "    pre_time = time.time()\n",
    "\n",
    "    # Variables for Early Stopping\n",
    "    is_best_model = 0\n",
    "    patient_epoch = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        discriminator.train()\n",
    "        # if use_gpu:\n",
    "        #     mem_allocated = torch.cuda.memory_allocated() / (1024 * 1024)  # MB単位で取得\n",
    "        #     print(f\"Epoch {epoch}: GPU memory allocated at start of epoch: {mem_allocated:.2f} MB\")\n",
    "\n",
    "        trained_number = 0\n",
    "\n",
    "        valid_dataloader_iter = iter(valid_dataloader)\n",
    "\n",
    "        losses_epoch_train = []\n",
    "        losses_epoch_valid = []\n",
    "\n",
    "        losses_epoch_d_loss_real = []\n",
    "        losses_epoch_d_loss_fake = []\n",
    "\n",
    "        # if use_gpu:\n",
    "        #     mem_allocated = torch.cuda.memory_allocated() / (1024 * 1024)  # MB単位で取得\n",
    "        #     print(f\"Epoch {epoch}: GPU memory allocated at before train: {mem_allocated:.2f} MB\")\n",
    "\n",
    "        i = 0 \n",
    "\n",
    "        for data in train_dataloader:\n",
    "            model.train()\n",
    "            discriminator.train()\n",
    "            inputs, labels = data\n",
    "\n",
    "            if inputs.shape[0] != batch_size:\n",
    "                continue\n",
    "\n",
    "            if use_gpu:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            else:\n",
    "                inputs, labels = inputs, labels\n",
    "\n",
    "            # print(\"inputs\")\n",
    "            # print(inputs.shape)\n",
    "\n",
    "            optimizer_adv.zero_grad()\n",
    "\n",
    "            forecasts, generation = model(inputs)\n",
    "            real_predictions = discriminator(torch.squeeze(labels[:, 0, :, :]))\n",
    "            fake_predictions = discriminator(generation.detach())\n",
    "\n",
    "            d_loss_real = adversarial_loss(real_predictions, torch.ones_like(real_predictions))\n",
    "            d_loss_fake = adversarial_loss(fake_predictions, torch.ones_like(fake_predictions))\n",
    "\n",
    "            d_loss = -d_loss_real + d_loss_fake\n",
    "            # print(f\"d_loss_real: {d_loss_real}\")\n",
    "            # print(f\"d_loss_fake: {d_loss_fake}\")\n",
    "\n",
    "            losses_epoch_d_loss_real.append(d_loss_real.data)\n",
    "            losses_epoch_d_loss_fake.append(d_loss_fake.data)\n",
    "\n",
    "            d_loss.backward()\n",
    "\n",
    "            optimizer_adv.step()\n",
    "\n",
    "            # print(\"forecasts\")\n",
    "            # print(forecasts.shape)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forecasting\n",
    "            outputs, generation = model(inputs)\n",
    "\n",
    "            forecasts_prediction = discriminator(generation.detach())\n",
    "            g_loss_forecast = adversarial_loss(forecasts_prediction, torch.ones_like(real_predictions))\n",
    "\n",
    "            outputs = torch.mul(outputs, torch.squeeze(labels[:, 1, :, :]))\n",
    "\n",
    "            if output_last:\n",
    "                loss_train = loss_MSE(torch.squeeze(outputs), torch.squeeze(labels[:, 0, :, :])) - lambda_dis * g_loss_forecast\n",
    "            else:\n",
    "                full_labels = torch.cat((inputs[:, 1:, :], labels), dim=1)\n",
    "                loss_train = loss_MSE(outputs, full_labels) - lambda_dis * g_loss_forecast\n",
    "\n",
    "            losses_train.append(loss_train.data)\n",
    "            losses_epoch_train.append(loss_train.data)\n",
    "\n",
    "            # print(f\"loss_train: {loss_train}\")\n",
    "\n",
    "            loss_train.backward()\n",
    "\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(model.memory)\n",
    "\n",
    "            # print(\n",
    "            #     f\"Epoch [{epoch}]  D Loss Real: {d_loss_real.item():.4f}  D Loss Fake: {d_loss_fake.item():.4f}  D Loss: {d_loss.item():.4f}\"\n",
    "            # )\n",
    "            # print(\n",
    "            #     f\"Forecasting Loss: {loss.item():.4f}  G Loss Forecast: {g_loss_forecast.item():.4f}  G Loss: {g_loss.item():.4f}\"\n",
    "            # )\n",
    "\n",
    "            if i < 10 and epoch % 10 ==0:\n",
    "                plot_locals_graph(model.local_statistics, epoch)\n",
    "                plot_local_input_graph(model.z, model.z_prime, model.x_i, 0, epoch)\n",
    "                i += 1\n",
    "\n",
    "            # validation\n",
    "            model.eval()\n",
    "            try:\n",
    "                inputs_val, labels_val = next(valid_dataloader_iter)\n",
    "            except StopIteration:\n",
    "                valid_dataloader_iter = iter(valid_dataloader)\n",
    "                inputs_val, labels_val = next(valid_dataloader_iter)\n",
    "\n",
    "            if use_gpu:\n",
    "                inputs_val, labels_val = inputs_val.cuda(), labels_val.cuda()\n",
    "            else:\n",
    "                inputs_val, labels_val = inputs_val, labels_val\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs_val, generation = model(inputs_val)\n",
    "                outputs_val = torch.mul(outputs_val, torch.squeeze(labels_val[:, 1, :, :]))\n",
    "\n",
    "                if output_last:\n",
    "                    loss_valid = loss_MSE(torch.squeeze(outputs_val), torch.squeeze(labels_val[:, 0, :, :]))\n",
    "                else:\n",
    "                    full_labels_val = torch.cat((inputs_val[:, 1:, :], labels_val), dim=1)\n",
    "                    loss_valid = loss_MSE(outputs_val, full_labels_val)\n",
    "\n",
    "            losses_valid.append(loss_valid.data)\n",
    "            losses_epoch_valid.append(loss_valid.data)\n",
    "\n",
    "            del inputs_val, labels_val, outputs_val, loss_valid, data\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # if use_gpu:\n",
    "            #     mem_allocated = torch.cuda.memory_allocated() / (1024 * 1024)  # MB単位で取得\n",
    "            #     print(\n",
    "            #         f\"Epoch {epoch}, Step {trained_number}: GPU memory allocated after validation step: {mem_allocated:.2f} MB\"\n",
    "            #     )\n",
    "\n",
    "            # output\n",
    "            trained_number += 1\n",
    "\n",
    "        avg_losses_epoch_train = sum(losses_epoch_train).cpu().numpy() / float(len(losses_epoch_train))\n",
    "        avg_losses_epoch_valid = sum(losses_epoch_valid).cpu().numpy() / float(len(losses_epoch_valid))\n",
    "        losses_epochs_train.append(avg_losses_epoch_train)\n",
    "        losses_epochs_valid.append(avg_losses_epoch_valid)\n",
    "\n",
    "        avg_losses_epoch_d_loss_real = sum(losses_epoch_d_loss_real).cpu().numpy() / float(\n",
    "            len(losses_epoch_d_loss_real)\n",
    "        )\n",
    "        avg_losses_epoch_d_loss_fake = sum(losses_epoch_d_loss_fake).cpu().numpy() / float(\n",
    "            len(losses_epoch_d_loss_fake)\n",
    "        )\n",
    "        losses_epochs_d_loss_real.append(avg_losses_epoch_d_loss_real)\n",
    "        losses_epochs_d_loss_fake.append(avg_losses_epoch_d_loss_fake)\n",
    "\n",
    "        # Early Stopping\n",
    "        if epoch == 0:\n",
    "            is_best_model = 1\n",
    "            best_model = model\n",
    "            min_loss_epoch_valid = 10000.0\n",
    "            if avg_losses_epoch_valid < min_loss_epoch_valid:\n",
    "                min_loss_epoch_valid = avg_losses_epoch_valid\n",
    "        else:\n",
    "            if min_loss_epoch_valid - avg_losses_epoch_valid > min_delta:\n",
    "                is_best_model = 1\n",
    "                best_model = model\n",
    "                min_loss_epoch_valid = avg_losses_epoch_valid\n",
    "                patient_epoch = 0\n",
    "            else:\n",
    "                is_best_model = 0\n",
    "                patient_epoch += 1\n",
    "                if patient_epoch >= patience:\n",
    "                    print(\"Early Stopped at Epoch:\", epoch)\n",
    "                    break\n",
    "\n",
    "        # Print training parameters\n",
    "        cur_time = time.time()\n",
    "        print(\n",
    "            \"Epoch: {}, train_loss: {}, d_loss_real: {}, d_loss_fake: {}, valid_loss: {}, time: {}, best model: {}\".format(\n",
    "                epoch,\n",
    "                np.around(avg_losses_epoch_train, decimals=8),\n",
    "                np.around(avg_losses_epoch_d_loss_real, decimals=8),\n",
    "                np.around(avg_losses_epoch_d_loss_fake, decimals=8),\n",
    "                np.around(avg_losses_epoch_valid, decimals=8),\n",
    "                np.around([cur_time - pre_time], decimals=2),\n",
    "                is_best_model,\n",
    "            )\n",
    "        )\n",
    "        pre_time = cur_time\n",
    "\n",
    "        if use_gpu:\n",
    "            mem_allocated = torch.cuda.memory_allocated() / (1024 * 1024)  # MB単位で取得\n",
    "            print(f\"Epoch {epoch}: GPU memory allocated at end of epoch: {mem_allocated:.2f} MB\")\n",
    "\n",
    "    plot_losses_combined(\n",
    "        losses_epochs_train,\n",
    "        losses_epochs_valid,\n",
    "        losses_epochs_d_loss_real,\n",
    "        losses_epochs_d_loss_fake,\n",
    "        \"combined_losses_mem.png\",\n",
    "    )\n",
    "\n",
    "    return best_model, [losses_train, losses_valid, losses_epochs_train, losses_epochs_valid]\n",
    "\n",
    "\n",
    "def Test_Model(model, test_dataloader, max_speed):\n",
    "    if type(model) == nn.modules.container.Sequential:\n",
    "        output_last = model[-1].output_last\n",
    "    else:\n",
    "        output_last = model.output_last\n",
    "\n",
    "    inputs, labels = next(iter(test_dataloader))\n",
    "    [batch_size, type_size, step_size, fea_size] = inputs.size()\n",
    "\n",
    "    cur_time = time.time()\n",
    "    pre_time = time.time()\n",
    "\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "\n",
    "    loss_MSE = torch.nn.MSELoss()\n",
    "    loss_L1 = torch.nn.MSELoss()\n",
    "\n",
    "    tested_batch = 0\n",
    "\n",
    "    losses_mse = []\n",
    "    losses_l1 = []\n",
    "    MAEs = []\n",
    "    MAPEs = []\n",
    "\n",
    "    for data in test_dataloader:\n",
    "        inputs, labels = data\n",
    "\n",
    "        if inputs.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        if use_gpu:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        else:\n",
    "            inputs, labels = inputs, labels\n",
    "\n",
    "        outputs, generation = model(inputs)\n",
    "\n",
    "        loss_MSE = torch.nn.MSELoss()\n",
    "        loss_L1 = torch.nn.L1Loss()\n",
    "\n",
    "        if output_last:\n",
    "            loss_mse = loss_MSE(torch.squeeze(outputs), torch.squeeze(labels[:, 0, :, :]))\n",
    "            loss_l1 = loss_L1(torch.squeeze(outputs), torch.squeeze(labels[:, 0, :, :]))\n",
    "            MAE = torch.mean(\n",
    "                torch.mul(\n",
    "                    torch.squeeze(labels[:, 1, :, :]),\n",
    "                    torch.abs(torch.squeeze(outputs) - torch.squeeze(labels[:, 0, :, :])),\n",
    "                )\n",
    "            )\n",
    "            MAPE = torch.mean(\n",
    "                torch.mul(\n",
    "                    torch.squeeze(labels[:, 1, :, :]),\n",
    "                    torch.abs(torch.squeeze(outputs) - torch.squeeze(labels[:, 0, :, :]))\n",
    "                    / torch.where(\n",
    "                        torch.squeeze(labels[:, 1, :, :]) == 0,\n",
    "                        torch.ones_like(torch.squeeze(labels[:, 0, :, :])),\n",
    "                        torch.squeeze(labels[:, 0, :, :]),\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            loss_mse = loss_MSE(outputs[:, -1, :], labels)\n",
    "            loss_l1 = loss_L1(outputs[:, -1, :], labels)\n",
    "            MAE = torch.mean(torch.abs(outputs[:, -1, :] - torch.squeeze(labels)))\n",
    "            MAPE = torch.mean(torch.abs(outputs[:, -1, :] - torch.squeeze(labels)) / torch.squeeze(labels))\n",
    "\n",
    "        losses_mse.append(loss_mse.data.cpu().numpy())  # Move to CPU and convert to NumPy array\n",
    "        losses_l1.append(loss_l1.data.cpu().numpy())  # Move to CPU and convert to NumPy array\n",
    "        MAEs.append(MAE.data.cpu().numpy())  # Move to CPU and convert to NumPy array\n",
    "        MAPEs.append(MAPE.data.cpu().numpy())  # Move to CPU and convert to NumPy array\n",
    "\n",
    "        tested_batch += 1\n",
    "\n",
    "        if tested_batch % 1000 == 0:\n",
    "            cur_time = time.time()\n",
    "            print(\n",
    "                \"Tested #: {}, loss_l1: {}, loss_mse: {}, time: {}\".format(\n",
    "                    tested_batch * batch_size,\n",
    "                    np.around([loss_l1.data[0]], decimals=8),\n",
    "                    np.around([loss_mse.data[0]], decimals=8),\n",
    "                    np.around([cur_time - pre_time], decimals=8),\n",
    "                )\n",
    "            )\n",
    "            pre_time = cur_time\n",
    "    losses_l1 = np.array(losses_l1)\n",
    "    losses_mse = np.array(losses_mse)\n",
    "    MAEs = np.array(MAEs)\n",
    "    MAPEs = np.array(MAPEs)\n",
    "\n",
    "    mean_l1 = np.mean(losses_l1) * max_speed\n",
    "    std_l1 = np.std(losses_l1) * max_speed\n",
    "    MAE_ = np.mean(MAEs) * max_speed\n",
    "    MAPE_ = np.mean(MAPEs) * 100\n",
    "\n",
    "    print(\"Tested: L1_mean: {}, L1_std: {}, MAE: {} MAPE: {}\".format(mean_l1, std_l1, MAE_, MAPE_))\n",
    "    return [losses_l1, losses_mse, mean_l1, std_l1]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = \"loop\"\n",
    "    if data == \"inrix\":\n",
    "        speed_matrix = pd.read_pickle(\"../Data_Warehouse/Data_network_traffic/inrix_seattle_speed_matrix_2012\")\n",
    "    elif data == \"loop\":\n",
    "        speed_matrix = pd.read_pickle(\"/workspaces/STdata_prediction/src/GRU-D-zhiyongc/input/speed_matrix_2015\")\n",
    "        np.random.seed(1024)\n",
    "        mask_ones_proportion=0.8\n",
    "        Mask = np.random.choice(\n",
    "            [0, 1], size=(speed_matrix.shape), p=[1 - mask_ones_proportion, mask_ones_proportion]\n",
    "        )\n",
    "        speed_matrix = np.multiply(speed_matrix, Mask)\n",
    "    elif data == \"LA\":\n",
    "        with h5py.File(\"/workspaces/STdata_prediction/src/LGnet/input/metr-la.h5\", \"r\") as f:\n",
    "            # dfグループ内のデータセットを取得\n",
    "            df_group = f[\"df\"]\n",
    "\n",
    "            # 各データセットを取得\n",
    "            axis0 = df_group[\"axis0\"][:]\n",
    "            axis1 = df_group[\"axis1\"][:]\n",
    "            block0_items = df_group[\"block0_items\"][:]\n",
    "            block0_values = df_group[\"block0_values\"][:]\n",
    "\n",
    "            # DataFrameの作成\n",
    "            speed_matrix = pd.DataFrame(block0_values, index=axis1, columns=block0_items)\n",
    "    elif data == \"BAY\":\n",
    "        with h5py.File(\"/workspaces/STdata_prediction/src/LGnet/input/pems-bay.h5\", \"r\") as f:\n",
    "            # dfグループ内のデータセットを取得\n",
    "            df_group = f[\"speed\"]\n",
    "\n",
    "            # 各データセットを取得\n",
    "            axis0 = df_group[\"axis0\"][:]\n",
    "            axis1 = df_group[\"axis1\"][:]\n",
    "            block0_items = df_group[\"block0_items\"][:]\n",
    "            block0_values = df_group[\"block0_values\"][:]\n",
    "\n",
    "            # DataFrameの作成\n",
    "            speed_matrix = pd.DataFrame(block0_values, index=axis1, columns=block0_items)\n",
    "\n",
    "    train_dataloader, valid_dataloader, test_dataloader, max_speed, X_mean = PrepareDataset(\n",
    "        speed_matrix, BATCH_SIZE=32, masking=True\n",
    "    )\n",
    "\n",
    "    inputs, labels = next(iter(train_dataloader))\n",
    "    [batch_size, type_size, step_size, fea_size] = inputs.size()\n",
    "    input_dim = fea_size\n",
    "    hidden_dim = fea_size\n",
    "    output_dim = fea_size\n",
    "\n",
    "    lgnet = LGnet_mem(\n",
    "        input_dim, hidden_dim, output_dim, X_mean, memory_size=64, memory_dim=128, num_layers=1, output_last=True\n",
    "    )\n",
    "    adv = Discriminator(input_dim)\n",
    "    best_lgnet, losses_lgnet = Train_Model(lgnet, adv, train_dataloader, valid_dataloader)\n",
    "    [losses_l1, losses_mse, mean_l1, std_l1] = Test_Model(best_lgnet, test_dataloader, max_speed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
