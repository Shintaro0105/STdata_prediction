{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "import itertools\n",
    "import numbers\n",
    "import torch.utils.data as utils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath = './input/set-a/'\n",
    "inputdict = {\n",
    "    \"ALP\" : 0,             # o\n",
    "    \"ALT\" : 1,             # o\n",
    "    \"AST\" : 2,             # o\n",
    "    \"Albumin\" : 3,         # o\n",
    "    \"BUN\" : 4,             # o\n",
    "    \"Bilirubin\" : 5,       # o\n",
    "    \"Cholesterol\" : 6,     # o\n",
    "    \"Creatinine\" : 7,      # o\n",
    "    \"DiasABP\" : 8,         # o\n",
    "    \"FiO2\" : 9,            # o\n",
    "    \"GCS\" : 10,            # o\n",
    "    \"Glucose\" : 11,        # o\n",
    "    \"HCO3\" : 12,           # o\n",
    "    \"HCT\" : 13,            # o\n",
    "    \"HR\" : 14,             # o\n",
    "    \"K\" : 15,              # o\n",
    "    \"Lactate\" : 16,        # o\n",
    "    \"MAP\" : 17,            # o\n",
    "    \"Mg\" : 18,             # o\n",
    "    \"Na\" : 19,             # o\n",
    "    \"PaCO2\" : 20,          # o\n",
    "    \"PaO2\" : 21,           # o\n",
    "    \"Platelets\" : 22,      # o\n",
    "    \"RespRate\" : 23,       # o\n",
    "    \"SaO2\" : 24,           # o\n",
    "    \"SysABP\" : 25,         # o\n",
    "    \"Temp\" : 26,           # o\n",
    "    \"Tropl\" : 27,          # o\n",
    "    \"TroponinI\" : 27,      # temp: regarded same as Tropl\n",
    "    \"TropT\" : 28,          # o\n",
    "    \"TroponinT\" : 28,      # temp: regarded same as TropT\n",
    "    \"Urine\" : 29,          # o\n",
    "    \"WBC\" : 30,            # o\n",
    "    \"Weight\" : 31,         # o\n",
    "    \"pH\" : 32,             # o\n",
    "    \"NIDiasABP\" : 33,      # unused variable\n",
    "    \"NIMAP\" : 34,          # unused variable\n",
    "    \"NISysABP\" : 35,       # unused variable\n",
    "    \"MechVent\" : 36,       # unused variable\n",
    "    \"RecordID\" : 37,       # unused variable\n",
    "    \"Age\" : 38,            # unused variable\n",
    "    \"Gender\" :39,          # unused variable\n",
    "    \"ICUType\" : 40,        # unused variable\n",
    "    \"Height\": 41           # unused variable\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to process the time in the data\n",
    "def timeparser(time):\n",
    "    return pd.to_timedelta(time + ':00')\n",
    "\n",
    "def timedelta_to_day_figure(timedelta):\n",
    "    return timedelta.days + (timedelta.seconds/86400) #(24*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the data by time\n",
    "def df_to_inputs(df, inputdict, inputs):\n",
    "    grouped_data = df.groupby('Time')\n",
    "            \n",
    "    for row_index, value in df.iterrows():\n",
    "        '''\n",
    "        t = colum ~ time frame\n",
    "        agg_no = row ~ variable\n",
    "        '''\n",
    "        \n",
    "        agg_no = inputdict[value.Parameter]\n",
    "\n",
    "        #print('agg_no : {}\\t  value : {}'.format(agg_no, value.Value))\n",
    "        inputs[agg_no].append(value.Value)    \n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "\n",
    "# prepare empty list to put data\n",
    "# len(inputdict)-2: two items has same agg_no\n",
    "for i in range(len(inputdict)-2):\n",
    "    t = []\n",
    "    inputs.append(t)\n",
    "\n",
    "# read all the files in the input folder\n",
    "for filename in os.listdir(inputpath):\n",
    "    df = pd.read_csv(inputpath + filename, header=0, \\\n",
    "                     parse_dates=['Time'], date_parser=timeparser)\n",
    "    \n",
    "    inputs = df_to_inputs(df=df, inputdict=inputdict, inputs=inputs)\n",
    "\n",
    "print(inputs[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object array to store the list of arrays\n",
    "object_inputs = np.empty(len(inputs), dtype=object)\n",
    "for i in range(len(inputs)):\n",
    "    object_inputs[i] = inputs[i]\n",
    "\n",
    "# Save and load using np.save and np.load with allow_pickle=True\n",
    "np.save('./input/inputs', object_inputs)\n",
    "loaded_inputs = np.load('./input/inputs.npy', allow_pickle=True)\n",
    "print(loaded_inputs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make input items list\n",
    "input_columns = list(inputdict.keys())\n",
    "\n",
    "'''\n",
    "remove two overlaped items\n",
    "\"TroponinI\" : 27, #temp\n",
    "\"TroponinT\" : 28, #temp\n",
    "\n",
    "'''\n",
    "input_columns.remove(\"TroponinI\")\n",
    "input_columns.remove(\"TroponinT\")\n",
    "print(input_columns)\n",
    "print(len(input_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the data\n",
    "# print count, min, max, mean, median, std, var and histogram if hist == True\n",
    "# return values as a list\n",
    "def describe(inputs, input_columns, inputdict, hist = False):\n",
    "    \n",
    "    desc = [] \n",
    "    \n",
    "    for i in range(len(inputdict)-2):\n",
    "        input_arr = np.asarray(inputs[i])\n",
    "        \n",
    "        des = []\n",
    "        \n",
    "        des.append(input_arr.size)\n",
    "        des.append(np.amin(input_arr))\n",
    "        des.append(np.amax(input_arr))\n",
    "        des.append(np.mean(input_arr))\n",
    "        des.append(np.median(input_arr))\n",
    "        des.append(np.std(input_arr))\n",
    "        des.append(np.var(input_arr))\n",
    "        \n",
    "        desc.append(des)\n",
    "        \n",
    "        # print histgram\n",
    "        if hist:\n",
    "            a = np.hstack(input_arr)\n",
    "            plt.hist(a, bins='auto')\n",
    "            plt.title(\"Histogram about {}\".format(input_columns[i]))\n",
    "            plt.show()\n",
    "        \n",
    "        print('count: {}, min: {}, max: {}'.format(des[0], des[1], des[2]))\n",
    "        print('mean: {}, median: {}, std: {}, var: {}'.format(des[3], des[4], des[5], des[6]))\n",
    "    \n",
    "    return desc       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = describe(loaded_inputs, input_columns, inputdict, hist=True)\n",
    "desc = np.asarray(desc)\n",
    "desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save desc\n",
    "# 0: count, 1: min, 2: max, 3: mean, 4: median, 5: std, 6: var\n",
    "np.save('./input/desc', desc)\n",
    "loaded_desc = np.load('./input/desc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(desc, inputs):\n",
    "    # for each catagory\n",
    "    for i in range(desc.shape[0]):\n",
    "        # for each value\n",
    "        for j in range(len(inputs[i])):\n",
    "            inputs[i][j] = (inputs[i][j] - desc[i][3])/desc[i][5]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dataframe to dataset\n",
    "'''\n",
    "\n",
    "def df_to_x_m_d(df, inputdict, size, id_posistion, split):\n",
    "    grouped_data = df.groupby('Time')\n",
    "    \n",
    "    #generate input vectors\n",
    "    x = np.zeros((len(inputdict)-2, grouped_data.ngroups))\n",
    "    masking = np.zeros((len(inputdict)-2, grouped_data.ngroups))\n",
    "    delta = np.zeros((split, size))\n",
    "    timetable = np.zeros(grouped_data.ngroups)\n",
    "    id = 0\n",
    "    \n",
    "    all_x = np.zeros((split,1))\n",
    "    \n",
    "    s_dataset = np.zeros((3, split, size))\n",
    "   \n",
    "    if grouped_data.ngroups > size:\n",
    "        \n",
    "        # fill the x and masking vectors\n",
    "        pre_time = pd.to_timedelta(0)\n",
    "        t = 0\n",
    "        for row_index, value in df.iterrows():\n",
    "            '''\n",
    "            t = colum, time frame\n",
    "            agg_no = row, variable\n",
    "            '''\n",
    "            #print(value)\n",
    "            agg_no = inputdict[value.Parameter]\n",
    "\n",
    "            # same timeline check.        \n",
    "            if pre_time != value.Time:\n",
    "                pre_time = value.Time\n",
    "                t += 1\n",
    "                timetable[t] = timedelta_to_day_figure(value.Time)\n",
    "\n",
    "            #print('agg_no : {}\\t t : {}\\t value : {}'.format(agg_no, t, value.Value))\n",
    "            x[agg_no, t] = value.Value    \n",
    "            masking[agg_no, t] = 1\n",
    "        \n",
    "        '''\n",
    "        # generate random index array \n",
    "        ran_index = np.random.choice(grouped_data.ngroups, size=size, replace=False)\n",
    "        ran_index.sort()\n",
    "        ran_index[0] = 0\n",
    "        ran_index[size-1] = grouped_data.ngroups-1\n",
    "        '''\n",
    "        \n",
    "        # generate index that has most parameters and first/last one.\n",
    "        ran_index = grouped_data.count()\n",
    "        ran_index = ran_index.reset_index()\n",
    "        ran_index = ran_index.sort_values('Value', ascending=False)\n",
    "        ran_index = ran_index[:size]\n",
    "        ran_index = ran_index.sort_index()\n",
    "        ran_index = np.asarray(ran_index.index.values)\n",
    "        ran_index[0] = 0\n",
    "        ran_index[size-1] = grouped_data.ngroups-1\n",
    "        \n",
    "        #print(ran_index)\n",
    "        \n",
    "        # take id for outcome comparing\n",
    "        id = x[id_posistion, 0]\n",
    "        \n",
    "        # remove unnesserly parts(rows)\n",
    "        x = x[:split, :]\n",
    "        masking = masking[:split, :]\n",
    "        \n",
    "        # coulme(time) sampling\n",
    "        x_sample = np.zeros((split, size))\n",
    "        m_sample = np.zeros((split, size))\n",
    "        time_sample = np.zeros(size)\n",
    "\n",
    "        t_x_sample = x_sample.T\n",
    "        t_marsking = m_sample.T\n",
    "        #t_time = t_sample.T\n",
    "        \n",
    "        t_x = x.T\n",
    "        t_m = masking.T\n",
    "        #t_t = t.T\n",
    "\n",
    "        it = np.nditer(ran_index, flags=['f_index'])\n",
    "        while not it.finished:\n",
    "            #print('it.index = {}, it[0] = {}, ran_index = {}'.format(it.index, it[0], ran_index[it.index]))\n",
    "            t_x_sample[it.index] = t_x[it[0]]\n",
    "            t_marsking[it.index] = t_m[it[0]]\n",
    "            time_sample[it.index] = timetable[it[0]]\n",
    "            it.iternext()\n",
    "        \n",
    "        x = x_sample\n",
    "        masking = m_sample\n",
    "        timetable = time_sample\n",
    "        '''\n",
    "        # normalize the X\n",
    "        nor_x = x/max_input[:, np.newaxis]\n",
    "        '''\n",
    "        # fill the delta vectors\n",
    "        for index, value in np.ndenumerate(masking):\n",
    "            '''\n",
    "            index[0] = row, agg\n",
    "            index[1] = col, time\n",
    "            '''\n",
    "            if index[1] == 0:\n",
    "                delta[index[0], index[1]] = 0\n",
    "            elif masking[index[0], index[1]-1] == 0:\n",
    "                delta[index[0], index[1]] = timetable[index[1]] - timetable[index[1]-1] + delta[index[0], index[1]-1]\n",
    "            else:\n",
    "                delta[index[0], index[1]] = timetable[index[1]] - timetable[index[1]-1]\n",
    "    \n",
    "    else:\n",
    "                \n",
    "        # fill the x and masking vectors\n",
    "        pre_time = pd.to_timedelta(0)\n",
    "        t = 0\n",
    "        for row_index, value in df.iterrows():\n",
    "            '''\n",
    "            t = colum, time frame\n",
    "            agg_no = row, variable\n",
    "            '''\n",
    "            #print(value)\n",
    "            agg_no = inputdict[value.Parameter]\n",
    "\n",
    "            # same timeline check.        \n",
    "            if pre_time != value.Time:\n",
    "                pre_time = value.Time\n",
    "                t += 1\n",
    "                timetable[t] = timedelta_to_day_figure(value.Time)\n",
    "\n",
    "            #print('agg_no : {}\\t t : {}\\t value : {}'.format(agg_no, t, value.Value))\n",
    "            x[agg_no, t] = value.Value    \n",
    "            masking[agg_no, t] = 1\n",
    "        \n",
    "        # take id for outcome comparing\n",
    "        id = x[id_posistion, 0]\n",
    "        \n",
    "        # remove unnesserly parts(rows)\n",
    "        x = x[:split, :]\n",
    "        masking = masking[:split, :]\n",
    "        \n",
    "        x = np.pad(x, ((0,0), (size-grouped_data.ngroups, 0)), 'constant')\n",
    "        masking = np.pad(masking, ((0,0), (size-grouped_data.ngroups, 0)), 'constant')\n",
    "        timetable = np.pad(timetable, (size-grouped_data.ngroups, 0), 'constant')\n",
    "        '''\n",
    "        # normalize the X\n",
    "        nor_x = x/max_input[:, np.newaxis]\n",
    "        '''\n",
    "        # fill the delta vectors\n",
    "        for index, value in np.ndenumerate(masking):\n",
    "            '''\n",
    "            index[0] = row, agg\n",
    "            index[1] = col, time\n",
    "            '''\n",
    "            if index[1] == 0:\n",
    "                delta[index[0], index[1]] = 0\n",
    "            elif masking[index[0], index[1]-1] == 0:\n",
    "                delta[index[0], index[1]] = timetable[index[1]] - timetable[index[1]-1] + delta[index[0], index[1]-1]\n",
    "            else:\n",
    "                delta[index[0], index[1]] = timetable[index[1]] - timetable[index[1]-1]\n",
    "    \n",
    "\n",
    "    all_x = np.concatenate((all_x, x), axis=1)\n",
    "    all_x = all_x[:,1:]\n",
    "    \n",
    "    s_dataset[0] = x\n",
    "    s_dataset[1] = masking\n",
    "    s_dataset[2] = delta\n",
    "    \n",
    "    return s_dataset, all_x, id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def df_to_x_m_d(df, inputdict, mean, std, size, id_posistion, split):\n",
    "size = 49 # steps ~ from the paper\n",
    "id_posistion = 37\n",
    "input_length = 33 # input variables ~ from the paper\n",
    "dataset = np.zeros((1,3, input_length, size))\n",
    "\n",
    "all_x_add = np.zeros((input_length,1))\n",
    "\n",
    "for filename in os.listdir(inputpath):\n",
    "    df = pd.read_csv(inputpath + filename,\\\n",
    "                     header=0,\\\n",
    "                     parse_dates=['Time'],\\\n",
    "                     date_parser=timeparser)\n",
    "    s_dataset, all_x, id = df_to_x_m_d(df=df, inputdict=inputdict, size=size, id_posistion=id_posistion, split=input_length)\n",
    "    \n",
    "    dataset = np.concatenate((dataset, s_dataset[np.newaxis, :,:,:]))\n",
    "    all_x_add = np.concatenate((all_x_add, all_x), axis=1)\n",
    "    \n",
    "\n",
    "dataset = dataset[1:, :,:,:]    \n",
    "# (total datasets, kind of data(x, masking, and delta), input length, num of varience)\n",
    "# (4000, 3, 33, 49)\n",
    "print(dataset.shape)\n",
    "print(dataset[0].shape)\n",
    "print(dataset[0][0][0])\n",
    "\n",
    "print(all_x_add.shape)\n",
    "all_x_add = all_x_add[:, 1:]\n",
    "print(all_x_add.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proportion = 0.8\n",
    "train_index = int(all_x_add.shape[1] * train_proportion)\n",
    "train_x = all_x_add[:, :train_index]\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean(x):\n",
    "    x_mean = []\n",
    "    for i in range(x.shape[0]):\n",
    "        mean = np.mean(x[i])\n",
    "        x_mean.append(mean)\n",
    "    return x_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median(x):\n",
    "    x_median = []\n",
    "    for i in range(x.shape[0]):\n",
    "        median = np.median(x[i])\n",
    "        x_median.append(median)\n",
    "    return x_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std(x):\n",
    "    x_std = []\n",
    "    for i in range(x.shape[0]):\n",
    "        std = np.std(x[i])\n",
    "        x_std.append(std)\n",
    "    return x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_var(x):\n",
    "    x_var = []\n",
    "    for i in range(x.shape[0]):\n",
    "        var = np.var(x[i])\n",
    "        x_var.append(var)\n",
    "    return x_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = get_mean(train_x)\n",
    "print(x_mean)\n",
    "print(len(x_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_std = get_std(train_x)\n",
    "print(x_std)\n",
    "print(len(x_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset shape : (4000, 3, 33, 49)\n",
    "def dataset_normalize(dataset, mean, std):\n",
    "    for i in range(dataset.shape[0]):        \n",
    "        dataset[i][0] = (dataset[i][0] - mean[:, None])\n",
    "        dataset[i][0] = dataset[i][0]/std[:, None]\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = np.asarray(x_mean)\n",
    "x_std = np.asarray(x_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_normalize(dataset=dataset, mean=x_mean, std=x_std)\n",
    "print(dataset[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_chk(dataset):\n",
    "    all_x_add = np.zeros((dataset[0][0].shape[0],1))\n",
    "    for i in range(dataset.shape[0]):\n",
    "        all_x_add = np.concatenate((all_x_add, dataset[i][0]), axis=1)\n",
    "    \n",
    "    mean = get_mean(all_x_add)\n",
    "    median = get_median(all_x_add)\n",
    "    std = get_std(all_x_add)\n",
    "    var = get_var(all_x_add)\n",
    "    \n",
    "    print('mean')\n",
    "    print(mean)\n",
    "    print('median')\n",
    "    print(median)\n",
    "    print('std')\n",
    "    print(std)\n",
    "    print('var')\n",
    "    print(var)\n",
    "    \n",
    "    return mean, median, std, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nor_mean, nor_median, nor_std, nor_var = normalize_chk(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./input/x_mean_aft_nor', nor_mean)\n",
    "np.save('./input/x_median_aft_nor', nor_median)\n",
    "np.save('./input/dataset', dataset)\n",
    "\n",
    "t_dataset = np.load('./input/dataset.npy')\n",
    "\n",
    "print(t_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Y values\n",
    "'''\n",
    "def df_to_y3(df):\n",
    "    '''\n",
    "    RecordID  SAPS-I  SOFA  Length_of_stay  Survival  In-hospital_death\n",
    "    '''\n",
    "    output = np.zeros((4000,3))\n",
    "    \n",
    "    for row_index, value in df.iterrows():\n",
    "        los = value[3] # Length_of_stay\n",
    "        sur = value[4] # Survival\n",
    "        ihd = value[5] # In-hospital_death\n",
    "\n",
    "        output[row_index][0] = ihd\n",
    "        output[row_index][1] = ihd\n",
    "\n",
    "        # length-of-stay less than 3 yes/no 1/0\n",
    "        if los < 3:\n",
    "            output[row_index][2] = 0\n",
    "        else:\n",
    "            output[row_index][2] = 1\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only check In-hospital_death\n",
    "def df_to_y1(df):\n",
    "    output = df.values\n",
    "    output = output[:,5:]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_outcomes = pd.read_csv('./input/Outcomes-a.txt')\n",
    "y1_outcomes = df_to_y1(A_outcomes)\n",
    "print(y1_outcomes.shape)\n",
    "np.save('./input/y1_out', y1_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_y2(df):\n",
    "    '''\n",
    "    RecordID  SAPS-I  SOFA  Length_of_stay  Survival  In-hospital_death\n",
    "    '''\n",
    "    output = np.zeros((4000,2))\n",
    "    \n",
    "    for row_index, value in df.iterrows():\n",
    "        ihd = value[5] # In-hospital_death\n",
    "\n",
    "        output[row_index][0] = ihd\n",
    "        output[row_index][1] = ihd\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_outcomes = pd.read_csv('./input/Outcomes-a.txt')\n",
    "y2_outcomes = df_to_y2(A_outcomes)\n",
    "print(y2_outcomes.shape)\n",
    "np.save('./input/y2_out', y2_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y2_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class GRUD(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, x_mean=0,\\\n",
    "                 bias=True, batch_first=False, bidirectional=False, dropout_type='mloss', dropout=0):\n",
    "        super(GRUD, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.zeros = torch.autograd.Variable(torch.zeros(input_size))\n",
    "        self.x_mean = torch.autograd.Variable(torch.tensor(x_mean))\n",
    "        self.bias = bias\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout_type = dropout_type\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        if not isinstance(dropout, numbers.Number) or not 0 <= dropout <= 1 or \\\n",
    "                isinstance(dropout, bool):\n",
    "            raise ValueError(\"dropout should be a number in range [0, 1] \"\n",
    "                             \"representing the probability of an element being \"\n",
    "                             \"zeroed\")\n",
    "        if dropout > 0 and num_layers == 1:\n",
    "            warnings.warn(\"dropout option adds dropout after all but last \"\n",
    "                          \"recurrent layer, so non-zero dropout expects \"\n",
    "                          \"num_layers greater than 1, but got dropout={} and \"\n",
    "                          \"num_layers={}\".format(dropout, num_layers))\n",
    "        \n",
    "        ################################\n",
    "        gate_size = 1 # not used\n",
    "        ################################\n",
    "        \n",
    "        self._all_weights = []\n",
    "\n",
    "        '''\n",
    "        w_ih = Parameter(torch.Tensor(gate_size, layer_input_size))\n",
    "        w_hh = Parameter(torch.Tensor(gate_size, hidden_size))\n",
    "        b_ih = Parameter(torch.Tensor(gate_size))\n",
    "        b_hh = Parameter(torch.Tensor(gate_size))\n",
    "        layer_params = (w_ih, w_hh, b_ih, b_hh)\n",
    "        '''\n",
    "        # decay rates gamma\n",
    "        w_dg_x = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "        w_dg_h = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # z\n",
    "        w_xz = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "        w_hz = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        w_mz = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "\n",
    "        # r\n",
    "        w_xr = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "        w_hr = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        w_mr = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "\n",
    "        # h_tilde\n",
    "        w_xh = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "        w_hh = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        w_mh = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "\n",
    "        # y (output)\n",
    "        w_hy = torch.nn.Parameter(torch.Tensor(output_size, hidden_size))\n",
    "\n",
    "        # bias\n",
    "        b_dg_x = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_dg_h = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_z = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_r = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_h = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_y = torch.nn.Parameter(torch.Tensor(output_size))\n",
    "\n",
    "        layer_params = (w_dg_x, w_dg_h,\\\n",
    "                        w_xz, w_hz, w_mz,\\\n",
    "                        w_xr, w_hr, w_mr,\\\n",
    "                        w_xh, w_hh, w_mh,\\\n",
    "                        w_hy,\\\n",
    "                        b_dg_x, b_dg_h, b_z, b_r, b_h, b_y)\n",
    "\n",
    "        param_names = ['weight_dg_x', 'weight_dg_h',\\\n",
    "                       'weight_xz', 'weight_hz','weight_mz',\\\n",
    "                       'weight_xr', 'weight_hr','weight_mr',\\\n",
    "                       'weight_xh', 'weight_hh','weight_mh',\\\n",
    "                       'weight_hy']\n",
    "        if bias:\n",
    "            param_names += ['bias_dg_x', 'bias_dg_h',\\\n",
    "                            'bias_z',\\\n",
    "                            'bias_r',\\\n",
    "                            'bias_h',\\\n",
    "                            'bias_y']\n",
    "        \n",
    "        for name, param in zip(param_names, layer_params):\n",
    "            setattr(self, name, param)\n",
    "        self._all_weights.append(param_names)\n",
    "\n",
    "        self.flatten_parameters()\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def flatten_parameters(self):\n",
    "        \"\"\"\n",
    "        Resets parameter data pointer so that they can use faster code paths.\n",
    "        Right now, this works only if the module is on the GPU and cuDNN is enabled.\n",
    "        Otherwise, it's a no-op.\n",
    "        \"\"\"\n",
    "        any_param = next(self.parameters()).data\n",
    "        if not any_param.is_cuda or not torch.backends.cudnn.is_acceptable(any_param):\n",
    "            return\n",
    "\n",
    "        # If any parameters alias, we fall back to the slower, copying code path. This is\n",
    "        # a sufficient check, because overlapping parameter buffers that don't completely\n",
    "        # alias would break the assumptions of the uniqueness check in\n",
    "        # Module.named_parameters().\n",
    "        all_weights = self._flat_weights\n",
    "        unique_data_ptrs = set(p.data_ptr() for p in all_weights)\n",
    "        if len(unique_data_ptrs) != len(all_weights):\n",
    "            return\n",
    "\n",
    "        with torch.cuda.device_of(any_param):\n",
    "            import torch.backends.cudnn.rnn as rnn\n",
    "\n",
    "            # NB: This is a temporary hack while we still don't have Tensor\n",
    "            # bindings for ATen functions\n",
    "            with torch.no_grad():\n",
    "                # NB: this is an INPLACE function on all_weights, that's why the\n",
    "                # no_grad() is necessary.\n",
    "                torch._cudnn_rnn_flatten_weight(\n",
    "                    all_weights, (4 if self.bias else 2),\n",
    "                    self.input_size, rnn.get_cudnn_mode(self.mode), self.hidden_size, self.num_layers,\n",
    "                    self.batch_first, bool(self.bidirectional))\n",
    "\n",
    "    def _apply(self, fn):\n",
    "        ret = super(GRUD, self)._apply(fn)\n",
    "        self.flatten_parameters()\n",
    "        return ret\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            torch.nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def check_forward_args(self, input, hidden, batch_sizes):\n",
    "        is_input_packed = batch_sizes is not None\n",
    "        expected_input_dim = 2 if is_input_packed else 3\n",
    "        if input.dim() != expected_input_dim:\n",
    "            raise RuntimeError(\n",
    "                'input must have {} dimensions, got {}'.format(\n",
    "                    expected_input_dim, input.dim()))\n",
    "        if self.input_size != input.size(-1):\n",
    "            raise RuntimeError(\n",
    "                'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n",
    "                    self.input_size, input.size(-1)))\n",
    "\n",
    "        if is_input_packed:\n",
    "            mini_batch = int(batch_sizes[0])\n",
    "        else:\n",
    "            mini_batch = input.size(0) if self.batch_first else input.size(1)\n",
    "\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        expected_hidden_size = (self.num_layers * num_directions,\n",
    "                                mini_batch, self.hidden_size)\n",
    "        \n",
    "        def check_hidden_size(hx, expected_hidden_size, msg='Expected hidden size {}, got {}'):\n",
    "            if tuple(hx.size()) != expected_hidden_size:\n",
    "                raise RuntimeError(msg.format(expected_hidden_size, tuple(hx.size())))\n",
    "\n",
    "        if self.mode == 'LSTM':\n",
    "            check_hidden_size(hidden[0], expected_hidden_size,\n",
    "                              'Expected hidden[0] size {}, got {}')\n",
    "            check_hidden_size(hidden[1], expected_hidden_size,\n",
    "                              'Expected hidden[1] size {}, got {}')\n",
    "        else:\n",
    "            check_hidden_size(hidden, expected_hidden_size)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        s = '{input_size}, {hidden_size}'\n",
    "        if self.num_layers != 1:\n",
    "            s += ', num_layers={num_layers}'\n",
    "        if self.bias is not True:\n",
    "            s += ', bias={bias}'\n",
    "        if self.batch_first is not False:\n",
    "            s += ', batch_first={batch_first}'\n",
    "        if self.dropout != 0:\n",
    "            s += ', dropout={dropout}'\n",
    "        if self.bidirectional is not False:\n",
    "            s += ', bidirectional={bidirectional}'\n",
    "        return s.format(**self.__dict__)\n",
    "    \n",
    "    \n",
    "    def __setstate__(self, d):\n",
    "        super(GRUD, self).__setstate__(d)\n",
    "        if 'all_weights' in d:\n",
    "            self._all_weights = d['all_weights']\n",
    "        if isinstance(self._all_weights[0][0], str):\n",
    "            return\n",
    "        num_layers = self.num_layers\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        self._all_weights = []\n",
    "\n",
    "        weights = ['weight_dg_x', 'weight_dg_h',\\\n",
    "                   'weight_xz', 'weight_hz','weight_mz',\\\n",
    "                   'weight_xr', 'weight_hr','weight_mr',\\\n",
    "                   'weight_xh', 'weight_hh','weight_mh',\\\n",
    "                   'weight_hy',\\\n",
    "                   'bias_dg_x', 'bias_dg_h',\\\n",
    "                   'bias_z', 'bias_r', 'bias_h','bias_y']\n",
    "\n",
    "        if self.bias:\n",
    "            self._all_weights += [weights]\n",
    "        else:\n",
    "            self._all_weights += [weights[:2]]\n",
    "\n",
    "    @property\n",
    "    def _flat_weights(self):\n",
    "        return list(self._parameters.values())\n",
    "\n",
    "    @property\n",
    "    def all_weights(self):\n",
    "        return [[getattr(self, weight) for weight in weights] for weights in self._all_weights]\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # input.size = (3, 33,49) : num_input or num_hidden, num_layer or step\n",
    "        X = torch.squeeze(input[0]) # .size = (33,49)\n",
    "        Mask = torch.squeeze(input[1]) # .size = (33,49)\n",
    "        Delta = torch.squeeze(input[2]) # .size = (33,49)\n",
    "        Hidden_State = torch.autograd.Variable(torch.zeros(input_size))\n",
    "        \n",
    "        step_size = X.size(1) # 49\n",
    "        #print('step size : ', step_size)\n",
    "        \n",
    "        output = None\n",
    "        h = Hidden_State\n",
    "\n",
    "        # decay rates gamma\n",
    "        w_dg_x = getattr(self, 'weight_dg_x')\n",
    "        w_dg_h = getattr(self, 'weight_dg_h')\n",
    "\n",
    "        #z\n",
    "        w_xz = getattr(self, 'weight_xz')\n",
    "        w_hz = getattr(self, 'weight_hz')\n",
    "        w_mz = getattr(self, 'weight_mz')\n",
    "\n",
    "        # r\n",
    "        w_xr = getattr(self, 'weight_xr')\n",
    "        w_hr = getattr(self, 'weight_hr')\n",
    "        w_mr = getattr(self, 'weight_mr')\n",
    "\n",
    "        # h_tilde\n",
    "        w_xh = getattr(self, 'weight_xh')\n",
    "        w_hh = getattr(self, 'weight_hh')\n",
    "        w_mh = getattr(self, 'weight_mh')\n",
    "\n",
    "        # bias\n",
    "        b_dg_x = getattr(self, 'bias_dg_x')\n",
    "        b_dg_h = getattr(self, 'bias_dg_h')\n",
    "        b_z = getattr(self, 'bias_z')\n",
    "        b_r = getattr(self, 'bias_r')\n",
    "        b_h = getattr(self, 'bias_h')\n",
    "        \n",
    "        for layer in range(num_layers):\n",
    "            \n",
    "            x = torch.squeeze(X[:,layer:layer+1])\n",
    "            m = torch.squeeze(Mask[:,layer:layer+1])\n",
    "            d = torch.squeeze(Delta[:,layer:layer+1])\n",
    "\n",
    "\n",
    "            #(4)\n",
    "            gamma_x = torch.exp(-torch.max(self.zeros, (w_dg_x * d + b_dg_x)))\n",
    "            gamma_h = torch.exp(-torch.max(self.zeros, (w_dg_h * d + b_dg_h)))\n",
    "\n",
    "            #(5)\n",
    "            x = m * x + (1 - m) * (gamma_x * x + (1 - gamma_x) * self.x_mean)\n",
    "\n",
    "            #(6)\n",
    "            if self.dropout == 0:\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "\n",
    "            elif self.dropout_type == 'Moon':\n",
    "                '''\n",
    "                RNNDROP: a novel dropout for rnn in asr(2015)\n",
    "                '''\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "                dropout = torch.nn.Dropout(p=self.dropout)\n",
    "                h = dropout(h)\n",
    "\n",
    "            elif self.dropout_type == 'Gal':\n",
    "                '''\n",
    "                A Theoretically grounded application of dropout in recurrent neural networks(2015)\n",
    "                '''\n",
    "                dropout = torch.nn.Dropout(p=self.dropout)\n",
    "                h = dropout(h)\n",
    "\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "\n",
    "            elif self.dropout_type == 'mloss':\n",
    "                '''\n",
    "                recurrent dropout without memory loss arXiv 1603.05118\n",
    "                g = h_tilde, p = the probability to not drop a neuron\n",
    "                '''\n",
    "\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                dropout = torch.nn.Dropout(p=self.dropout)\n",
    "                h_tilde = dropout(h_tilde)\n",
    "\n",
    "                h = (1 - z)* h + z*h_tilde\n",
    "\n",
    "            else:\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "\n",
    "            \n",
    "        w_hy = getattr(self, 'weight_hy')\n",
    "        b_y = getattr(self, 'bias_y')\n",
    "\n",
    "        output = torch.matmul(w_hy, h) + b_y\n",
    "        output = torch.sigmoid(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_dataloader(dataset, outcomes,\\\n",
    "                    train_proportion = 0.8, dev_proportion = 0.2, test_proportion = 0.2):\n",
    "    \n",
    "    train_index = int(np.floor(dataset.shape[0] * train_proportion))\n",
    "    dev_index = int(np.floor(dataset.shape[0] * (train_proportion - dev_proportion)))\n",
    "    \n",
    "    # split dataset to tarin/dev/test set\n",
    "    train_data, train_label = dataset[:train_index, :,:,:], outcomes[:train_index, :]\n",
    "    dev_data, dev_label = dataset[dev_index:train_index, :,:,:], outcomes[dev_index:train_index, :]\n",
    "    test_data, test_label = dataset[train_index:, :,:,:], outcomes[train_index:, :]   \n",
    "    \n",
    "    # ndarray to tensor\n",
    "    train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n",
    "    dev_data, dev_label = torch.Tensor(dev_data), torch.Tensor(dev_label)\n",
    "    test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
    "    \n",
    "    # tensor to dataset\n",
    "    train_dataset = utils.TensorDataset(train_data, train_label)\n",
    "    dev_dataset = utils.TensorDataset(dev_data, dev_label)\n",
    "    test_dataset = utils.TensorDataset(test_data, test_label)\n",
    "    \n",
    "    # dataset to dataloader \n",
    "    train_dataloader = utils.DataLoader(train_dataset)\n",
    "    dev_dataloader = utils.DataLoader(dev_dataset)\n",
    "    test_dataloader = utils.DataLoader(test_dataset)\n",
    "    \n",
    "    print(\"train_data.shape : {}\\t train_label.shape : {}\".format(train_data.shape, train_label.shape))\n",
    "    print(\"dev_data.shape : {}\\t dev_label.shape : {}\".format(dev_data.shape, dev_label.shape))\n",
    "    print(\"test_data.shape : {}\\t test_label.shape : {}\".format(test_data.shape, test_label.shape))\n",
    "    \n",
    "    return train_dataloader, dev_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataset = np.load('./input/dataset.npy')\n",
    "t_out = np.load('./input/y1_out.npy')\n",
    "\n",
    "print(t_dataset.shape)\n",
    "print(t_out.shape)\n",
    "\n",
    "train_dataloader, dev_dataloader, test_dataloader = data_dataloader(t_dataset, t_out, train_proportion=0.8, dev_proportion=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "in the paper : 49 layers, 33 input, 18838 parameters\n",
    "input : 10-weights(*input), 6 - biases\n",
    "Y: 1 weight(hidden*output), 1 bias(output)\n",
    "Input : hidden : output : layer  = # of parameters : len(para)\n",
    "1:1:1:1 = 18 : 18\n",
    "2:1:1:1 = 25 : 18  // +7 as expected\n",
    "1:1:1:2 = 34 : 18 // 34 = 16*2 + 2\n",
    "33:33:1:1 = 562 : 18 // 16*33(528) + 33*1 +1 = 562\n",
    "33:33:5:1 = 698 : 18 // 16*33(528) + 33*5(165) +5 = 698\n",
    "33:33:5:49 = 26042 : 18 // 16*33*49(25872) + 33*5(165) +5 = 698\n",
    "weights = 10*33*49(16170) + 33*5(165) = 16335 gap : 2503\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, criterion, learning_rate,\\\n",
    "        train_dataloader, dev_dataloader, test_dataloader,\\\n",
    "        learning_rate_decay=0, n_epochs=30):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # to check the update \n",
    "    old_state_dict = {}\n",
    "    for key in model.state_dict():\n",
    "        old_state_dict[key] = model.state_dict()[key].clone()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        if learning_rate_decay != 0:\n",
    "\n",
    "            # every [decay_step] epoch reduce the learning rate by half\n",
    "            if  epoch % learning_rate_decay == 0:\n",
    "                learning_rate = learning_rate/2\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                print('at epoch {} learning_rate is updated to {}'.format(epoch, learning_rate))\n",
    "        \n",
    "        # train the model\n",
    "        losses, acc = [], []\n",
    "        label, pred = [], []\n",
    "        y_pred_col= []\n",
    "        model.train()\n",
    "        for train_data, train_label in train_dataloader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Squeeze the data [1, 33, 49], [1,5] to [33, 49], [5]\n",
    "            train_data = torch.squeeze(train_data)\n",
    "            train_label = torch.squeeze(train_label)\n",
    "            \n",
    "            # Forward pass : Compute predicted y by passing train data to the model\n",
    "            y_pred = model(train_data)\n",
    "            \n",
    "            # y_pred = y_pred[:, None]\n",
    "            # train_label = train_label[:, None]\n",
    "            \n",
    "            #print(y_pred.shape)\n",
    "            #print(train_label.shape)\n",
    "\n",
    "            train_label = train_label.unsqueeze(1)\n",
    "            \n",
    "            # Save predict and label\n",
    "            y_pred_col.append(y_pred.item())\n",
    "            pred.append(y_pred.item() > 0.5)\n",
    "            label.append(train_label.item())\n",
    "            \n",
    "            #print('y_pred: {}\\t label: {}'.format(y_pred, train_label))\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(y_pred, train_label)\n",
    "            acc.append(\n",
    "                torch.eq(\n",
    "                    (torch.sigmoid(y_pred).data > 0.5).float(),\n",
    "                    train_label)\n",
    "            )\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # perform a backward pass, and update the weights.\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        \n",
    "        train_acc = torch.mean(torch.cat(acc).float())\n",
    "        train_loss = np.mean(losses)\n",
    "        \n",
    "        train_pred_out = pred\n",
    "        train_label_out = label\n",
    "        \n",
    "        # save new params\n",
    "        new_state_dict= {}\n",
    "        for key in model.state_dict():\n",
    "            new_state_dict[key] = model.state_dict()[key].clone()\n",
    "            \n",
    "        # compare params\n",
    "        for key in old_state_dict:\n",
    "            if (old_state_dict[key] == new_state_dict[key]).all():\n",
    "                print('Not updated in {}'.format(key))\n",
    "   \n",
    "        \n",
    "        # dev loss\n",
    "        losses, acc = [], []\n",
    "        label, pred = [], []\n",
    "        model.eval()\n",
    "        for dev_data, dev_label in dev_dataloader:\n",
    "            # Squeeze the data [1, 33, 49], [1,5] to [33, 49], [5]\n",
    "            dev_data = torch.squeeze(dev_data)\n",
    "            dev_label = torch.squeeze(dev_label)\n",
    "            \n",
    "            # Forward pass : Compute predicted y by passing train data to the model\n",
    "            y_pred = model(dev_data)\n",
    "            \n",
    "            # Save predict and label\n",
    "            pred.append(y_pred.item())\n",
    "            label.append(dev_label.item())\n",
    "\n",
    "            dev_label = dev_label.unsqueeze(1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(y_pred, dev_label)\n",
    "            acc.append(\n",
    "                torch.eq(\n",
    "                    (torch.sigmoid(y_pred).data > 0.5).float(),\n",
    "                    dev_label)\n",
    "            )\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "        dev_acc = torch.mean(torch.cat(acc).float())\n",
    "        dev_loss = np.mean(losses)\n",
    "        \n",
    "        dev_pred_out = pred\n",
    "        dev_label_out = label\n",
    "        \n",
    "        # test loss\n",
    "        losses, acc = [], []\n",
    "        label, pred = [], []\n",
    "        model.eval()\n",
    "        for test_data, test_label in test_dataloader:\n",
    "            # Squeeze the data [1, 33, 49], [1,5] to [33, 49], [5]\n",
    "            test_data = torch.squeeze(test_data)\n",
    "            test_label = torch.squeeze(test_label)\n",
    "            \n",
    "            # Forward pass : Compute predicted y by passing train data to the model\n",
    "            y_pred = model(test_data)\n",
    "            \n",
    "            # Save predict and label\n",
    "            pred.append(y_pred.item())\n",
    "            label.append(test_label.item())\n",
    "\n",
    "            test_label = test_label.unsqueeze(1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(y_pred, test_label)\n",
    "            acc.append(\n",
    "                torch.eq(\n",
    "                    (torch.sigmoid(y_pred).data > 0.5).float(),\n",
    "                    test_label)\n",
    "            )\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "        test_acc = torch.mean(torch.cat(acc).float())\n",
    "        test_loss = np.mean(losses)\n",
    "        \n",
    "        test_pred_out = pred\n",
    "        test_label_out = label\n",
    "                \n",
    "        epoch_losses.append([\n",
    "             train_loss, dev_loss, test_loss,\n",
    "             train_acc, dev_acc, test_acc,\n",
    "             train_pred_out, dev_pred_out, test_pred_out,\n",
    "             train_label_out, dev_label_out, test_label_out,\n",
    "         ])\n",
    "        \n",
    "        pred = np.asarray(pred)\n",
    "        label = np.asarray(label)\n",
    "        \n",
    "        auc_score = roc_auc_score(label, pred)\n",
    "        \n",
    "        # print(\"Epoch: {} Train: {:.4f}/{:.2f}%, Dev: {:.4f}/{:.2f}%, Test: {:.4f}/{:.2f}% AUC: {:.4f}\".format(\n",
    "        #     epoch, train_loss, train_acc*100, dev_loss, dev_acc*100, test_loss, test_acc*100, auc_score))\n",
    "        print(\"Epoch: {} Train loss: {:.4f}, Dev loss: {:.4f}, Test loss: {:.4f}, Test AUC: {:.4f}\".format(\n",
    "            epoch, train_loss, dev_loss, test_loss, auc_score))\n",
    "        \n",
    "        # save the parameters\n",
    "        train_log = []\n",
    "        train_log.append(model.state_dict())\n",
    "        torch.save(model.state_dict(), './save/grud_mean_grud_para.pt')\n",
    "        \n",
    "        #print(train_log)\n",
    "    \n",
    "    return epoch_losses      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_and_auc_score(outputs, labels, title):\n",
    "    false_positive_rate, true_positive_rate, threshold = roc_curve(labels, outputs)\n",
    "    auc_score = roc_auc_score(labels, outputs)\n",
    "    plt.plot(false_positive_rate, true_positive_rate, label = 'ROC curve, AREA = {:.4f}'.format(auc_score))\n",
    "    plt.plot([0,1], [0,1], 'red')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.title(title)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 33 # num of variables base on the paper\n",
    "hidden_size = 33 # same as inputsize\n",
    "output_size = 1\n",
    "num_layers = 49 # num of step or layers base on the paper\n",
    "\n",
    "x_mean = torch.Tensor(np.load('./input/x_mean_aft_nor.npy'))\n",
    "x_median = torch.Tensor(np.load('./input/x_median_aft_nor.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout_type : Moon, Gal, mloss\n",
    "model = GRUD(input_size = input_size, hidden_size= hidden_size, output_size=output_size, dropout=0, dropout_type='mloss', x_mean=x_mean, num_layers=num_layers)\n",
    "\n",
    "# load the parameters\n",
    "# model.load_state_dict(torch.load('./save/grud_para.pt'))\n",
    "# model.eval()\n",
    "\n",
    "count = count_parameters(model)\n",
    "print('number of parameters : ' , count)\n",
    "print(list(model.parameters())[0].grad)\n",
    "\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def fit(model, criterion, learning_rate,\\\n",
    "        train_dataloader, dev_dataloader, test_dataloader,\\\n",
    "        learning_rate_decay=0, n_epochs=30):\n",
    "'''\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay =7 \n",
    "n_epochs = 14\n",
    "\n",
    "# learning_rate = 0.1 learning_rate_decay=True\n",
    "epoch_losses = fit(model, criterion, learning_rate,\\\n",
    "                   train_dataloader, dev_dataloader, test_dataloader,\\\n",
    "                   learning_rate_decay, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "learning_rate_decay =7 \n",
    "n_epochs = 14\n",
    "\n",
    "# learning_rate = 0.1 learning_rate_decay=True\n",
    "epoch_losses_s = fit(model, criterion, learning_rate,\\\n",
    "                   train_dataloader, dev_dataloader, test_dataloader,\\\n",
    "                   learning_rate_decay, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "learning_rate_decay =7 \n",
    "n_epochs = 14\n",
    "\n",
    "# learning_rate = 0.1 learning_rate_decay=True\n",
    "epoch_losses_t = fit(model, criterion, learning_rate,\\\n",
    "                   train_dataloader, dev_dataloader, test_dataloader,\\\n",
    "                   learning_rate_decay, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds , test_labels = epoch_losses_t[1][8], epoch_losses_s[1][11]\n",
    "\n",
    "plot_roc_and_auc_score(test_preds, test_labels, 'GRU-D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
